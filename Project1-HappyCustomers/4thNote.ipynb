{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1lyLnTtC3N3K2jy_nPqYk4x3muGINGQ2u",
      "authorship_tag": "ABX9TyNeXv9vssd2gQUoT28OK3Kf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinnew99/Apziva-Projects/blob/main/Project1-HappyCustomers/4thNote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Suggestions for the Last Try:\n",
        "1. Use Recursive Elimination with XGBClassifier Model, see if I can achieve similar performance for each 3, 4, 5 features. <-- Because I need to know which one impacts for the consequences\n",
        "\n",
        "2. Write a 'Good Conlcusion' for this project\n",
        "- inlcuding the key take aways (including what should I get from this project, what would I recommend to the company)\n",
        "\n",
        "3. Submit to GitHub (public), structure it well, submit the final version (documentation and everyrthing, and finally submit to the Apziva Platform) => so that my mentor can 'complete' and go to the next step\n",
        "\n",
        "4. FINALLY! Try to reach out more often (<-- I need to improve this!)\n"
      ],
      "metadata": {
        "id": "fNKL5aznjrRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA_jxDT2MPXA",
        "outputId": "03312bf0-5793-45ce-edb7-2e1f220c743d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict"
      ],
      "metadata": {
        "id": "tx3tRxo9nJGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4c5d9d-c8d4-42f1-b011-b9f88f3f9bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lazypredict in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.4.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.4.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lazypredict) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost->lazypredict) (2.22.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from xgboost import XGBClassifier\n",
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from hyperopt.pyll.base import scope\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "BS7KWN_6k6hw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bd31dd-fd09-4d5f-f876-840751c7b985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Apziva/ACME-HappinessSurvey2020.csv')\n",
        "\n",
        "data = df[['X1', 'X2', 'X3', 'X4', 'X5', 'X6']]\n",
        "target = df[['Y']]"
      ],
      "metadata": {
        "id": "9DnaoAjbnVoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed = random.randint(1000, 9999)\n",
        "seed = 6245\n",
        "print(seed)  # 6245, XGB - racll (0.88) for class 0\n",
        "\n",
        "# run quite a few times, monitor each time to find out better seeds and whether it impacts higher perfermance on class 0, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykSparmFnagR",
        "outputId": "b799466e-cc00-4589-f344-26d292d51bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=seed)"
      ],
      "metadata": {
        "id": "gvYKWo3JnkYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LazyClassifier\n",
        "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
        "print(models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85NXeg_3noBS",
        "outputId": "662f93f1-33ac-4acf-8634-8c8e66ff41a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:01<00:00, 15.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 51, number of negative: 49\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 31\n",
            "[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.510000 -> initscore=0.040005\n",
            "[LightGBM] [Info] Start training from score 0.040005\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
            "Model                                                                           \n",
            "LabelPropagation                   0.69               0.71     0.71      0.70   \n",
            "LabelSpreading                     0.69               0.71     0.71      0.70   \n",
            "BernoulliNB                        0.62               0.69     0.69      0.62   \n",
            "LGBMClassifier                     0.58               0.66     0.66      0.58   \n",
            "BaggingClassifier                  0.65               0.65     0.65      0.67   \n",
            "DecisionTreeClassifier             0.65               0.65     0.65      0.67   \n",
            "QuadraticDiscriminantAnalysis      0.58               0.62     0.62      0.59   \n",
            "NuSVC                              0.65               0.61     0.61      0.66   \n",
            "GaussianNB                         0.54               0.60     0.60      0.55   \n",
            "SVC                                0.54               0.60     0.60      0.55   \n",
            "NearestCentroid                    0.54               0.60     0.60      0.55   \n",
            "ExtraTreeClassifier                0.58               0.56     0.56      0.59   \n",
            "RidgeClassifierCV                  0.50               0.53     0.53      0.52   \n",
            "LinearSVC                          0.50               0.53     0.53      0.52   \n",
            "CalibratedClassifierCV             0.50               0.53     0.53      0.52   \n",
            "AdaBoostClassifier                 0.50               0.53     0.53      0.52   \n",
            "RidgeClassifier                    0.50               0.53     0.53      0.52   \n",
            "LogisticRegression                 0.50               0.53     0.53      0.52   \n",
            "LinearDiscriminantAnalysis         0.50               0.53     0.53      0.52   \n",
            "XGBClassifier                      0.54               0.53     0.53      0.56   \n",
            "RandomForestClassifier             0.54               0.53     0.53      0.56   \n",
            "DummyClassifier                    0.69               0.50     0.50      0.57   \n",
            "ExtraTreesClassifier               0.54               0.49     0.49      0.55   \n",
            "SGDClassifier                      0.35               0.49     0.49      0.27   \n",
            "Perceptron                         0.50               0.47     0.47      0.52   \n",
            "KNeighborsClassifier               0.42               0.34     0.34      0.43   \n",
            "PassiveAggressiveClassifier        0.27               0.30     0.30      0.28   \n",
            "\n",
            "                               Time Taken  \n",
            "Model                                      \n",
            "LabelPropagation                     0.04  \n",
            "LabelSpreading                       0.03  \n",
            "BernoulliNB                          0.03  \n",
            "LGBMClassifier                       0.11  \n",
            "BaggingClassifier                    0.06  \n",
            "DecisionTreeClassifier               0.03  \n",
            "QuadraticDiscriminantAnalysis        0.03  \n",
            "NuSVC                                0.03  \n",
            "GaussianNB                           0.02  \n",
            "SVC                                  0.07  \n",
            "NearestCentroid                      0.03  \n",
            "ExtraTreeClassifier                  0.03  \n",
            "RidgeClassifierCV                    0.02  \n",
            "LinearSVC                            0.04  \n",
            "CalibratedClassifierCV               0.08  \n",
            "AdaBoostClassifier                   0.26  \n",
            "RidgeClassifier                      0.02  \n",
            "LogisticRegression                   0.03  \n",
            "LinearDiscriminantAnalysis           0.04  \n",
            "XGBClassifier                        0.24  \n",
            "RandomForestClassifier               0.22  \n",
            "DummyClassifier                      0.03  \n",
            "ExtraTreesClassifier                 0.15  \n",
            "SGDClassifier                        0.03  \n",
            "Perceptron                           0.04  \n",
            "KNeighborsClassifier                 0.02  \n",
            "PassiveAggressiveClassifier          0.03  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Feature Elimination (RFE)"
      ],
      "metadata": {
        "id": "K3HCrRnbk2DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive Feature Elimination (RFE)\n",
        "def perform_rfe(model, X_train, y_train, k):\n",
        "    rfe = RFE(estimator=model, n_features_to_select=k)\n",
        "    fit = rfe.fit(X_train, y_train.values.ravel())\n",
        "    return fit"
      ],
      "metadata": {
        "id": "OwUgjw5nnqyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb_rfe = XGBClassifier()  # make sure I'm using the same seeds for the models"
      ],
      "metadata": {
        "id": "LIzCXDQsXzub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-3 Features"
      ],
      "metadata": {
        "id": "5lPJHzafcvnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k features based on RFE\n",
        "rfe3 = RFE(model_xgb_rfe, n_features_to_select=3)\n",
        "rfe3.fit(X_train, y_train.values.ravel())\n",
        "# Print out the top-k features based on RFE\n",
        "top_3_features = data.columns[rfe3.support_]\n",
        "print(f'Top-{len(top_3_features)} features selected by RFE: {list(top_3_features)}')"
      ],
      "metadata": {
        "id": "9cv9I3GcnsZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19b4763-9d12-42a4-8674-72bdced831b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-3 features selected by RFE: ['X1', 'X5', 'X6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the training and test sets with the selected features\n",
        "X_train_rfe3 = rfe3.transform(X_train)\n",
        "X_test_rfe3 = rfe3.transform(X_test)"
      ],
      "metadata": {
        "id": "rrwr5e0hXeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning using HyperOpt for XGBClassifier  # Also provide the same seeds  #\n",
        "def hyperopt_train_test(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    return cross_val_score(clf, X_train_rfe3, y_train.values.ravel(), scoring='recall').mean()\n",
        "\n",
        "space4xgb = {\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 20, 1)),\n",
        "    'n_estimators': scope.int(hp.quniform('n_estimators', 10, 200, 1)),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n",
        "}"
      ],
      "metadata": {
        "id": "HiKVE3CvlHJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(params):\n",
        "    acc = hyperopt_train_test(params)\n",
        "    return {'loss': -acc, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(f, space4xgb, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "print('Best hyperparameters:', best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6t5Mj6QlKbz",
        "outputId": "da78ad10-a06c-4a86-8536-be117a88cb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [00:26<00:00,  1.88trial/s, best loss: -0.7090909090909092]\n",
            "Best hyperparameters: {'learning_rate': 0.16742088629517582, 'max_depth': 3.0, 'n_estimators': 64.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBClassifier with best hyperparameters\n",
        "best_params = {\n",
        "    'max_depth': int(best['max_depth']),\n",
        "    'n_estimators': int(best['n_estimators']),\n",
        "    'learning_rate': best['learning_rate']\n",
        "}"
      ],
      "metadata": {
        "id": "oTJz3HnvlQHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb3 = XGBClassifier(**best_params)\n",
        "model_xgb3.fit(X_train_rfe3, y_train.values.ravel())\n",
        "predictions_xgb3 = model_xgb3.predict(X_test_rfe3)"
      ],
      "metadata": {
        "id": "k8v12kL2lrT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate XGBClassifier\n",
        "accuracy_xgb = accuracy_score(y_test, predictions_xgb3)\n",
        "conf_matrix_xgb3 = confusion_matrix(y_test, predictions_xgb3)\n",
        "class_report_xgb3 = classification_report(y_test, predictions_xgb3)"
      ],
      "metadata": {
        "id": "heraOvcxlpyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'XGBClassifier Accuracy: {accuracy_xgb}')\n",
        "print('XGBClassifier Confusion Matrix:')\n",
        "print(conf_matrix_xgb3)\n",
        "print('XGBClassifier Classification Report:')\n",
        "print(class_report_xgb3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wifHvntqljuL",
        "outputId": "d86bec39-c4bf-42b9-8592-4517b4cc7f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier Accuracy: 0.7307692307692307\n",
            "XGBClassifier Confusion Matrix:\n",
            "[[ 6  2]\n",
            " [ 5 13]]\n",
            "XGBClassifier Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.75      0.63         8\n",
            "           1       0.87      0.72      0.79        18\n",
            "\n",
            "    accuracy                           0.73        26\n",
            "   macro avg       0.71      0.74      0.71        26\n",
            "weighted avg       0.77      0.73      0.74        26\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-4 Features"
      ],
      "metadata": {
        "id": "rcVRaqfacy0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k features based on RFE\n",
        "rfe4 = RFE(model_xgb_rfe, n_features_to_select=4)\n",
        "rfe4.fit(X_train, y_train.values.ravel())\n",
        "# Print out the top-k features based on RFE\n",
        "top_4_features = data.columns[rfe4.support_]\n",
        "print(f'Top-{len(top_4_features)} features selected by RFE: {list(top_4_features)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z1kqDdrXX78",
        "outputId": "fc367a2d-0eee-4454-83a3-2408cb0af0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-4 features selected by RFE: ['X1', 'X3', 'X5', 'X6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the training and test sets with the selected features\n",
        "X_train_rfe4 = rfe4.transform(X_train)\n",
        "X_test_rfe4 = rfe4.transform(X_test)"
      ],
      "metadata": {
        "id": "85wkKEMspeJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning using HyperOpt for XGBClassifier  # Also provide the same seeds  #\n",
        "def hyperopt_train_test(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    return cross_val_score(clf, X_train_rfe4, y_train.values.ravel(), scoring='recall').mean()\n",
        "\n",
        "space4xgb = {\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 20, 1)),\n",
        "    'n_estimators': scope.int(hp.quniform('n_estimators', 10, 200, 1)),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n",
        "}"
      ],
      "metadata": {
        "id": "s9W9wPN7pkB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(params):\n",
        "    acc = hyperopt_train_test(params)\n",
        "    return {'loss': -acc, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(f, space4xgb, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "print('Best hyperparameters:', best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ephFpoZBpn87",
        "outputId": "5b907313-0cf0-4a8e-e698-ba5941b1ca1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [00:06<00:00,  7.19trial/s, best loss: -0.7290909090909091]\n",
            "Best hyperparameters: {'learning_rate': 0.13055370869668198, 'max_depth': 2.0, 'n_estimators': 170.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb4 = XGBClassifier(**best_params)\n",
        "model_xgb4.fit(X_train_rfe4, y_train.values.ravel())\n",
        "predictions_xgb4 = model_xgb4.predict(X_test_rfe4)"
      ],
      "metadata": {
        "id": "OqPDavQtpqaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate XGBClassifier\n",
        "accuracy_xgb4 = accuracy_score(y_test, predictions_xgb4)\n",
        "conf_matrix_xgb4 = confusion_matrix(y_test, predictions_xgb4)\n",
        "class_report_xgb4 = classification_report(y_test, predictions_xgb4)"
      ],
      "metadata": {
        "id": "JJqBTh2mpzw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'XGBClassifier Accuracy: {accuracy_xgb4}')\n",
        "print('XGBClassifier Confusion Matrix:')\n",
        "print(conf_matrix_xgb4)\n",
        "print('XGBClassifier Classification Report:')\n",
        "print(class_report_xgb4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zGUm5rwqCRj",
        "outputId": "837ab3f5-5ced-4529-c3c8-4fc335c88c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier Accuracy: 0.6153846153846154\n",
            "XGBClassifier Confusion Matrix:\n",
            "[[ 4  4]\n",
            " [ 6 12]]\n",
            "XGBClassifier Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.50      0.44         8\n",
            "           1       0.75      0.67      0.71        18\n",
            "\n",
            "    accuracy                           0.62        26\n",
            "   macro avg       0.57      0.58      0.58        26\n",
            "weighted avg       0.64      0.62      0.63        26\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-5 Features"
      ],
      "metadata": {
        "id": "aRmwOAQJc4A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k features based on RFE\n",
        "rfe5 = RFE(model_xgb_rfe, n_features_to_select=5)\n",
        "rfe5.fit(X_train, y_train.values.ravel())\n",
        "# Print out the top-k features based on RFE\n",
        "top_5_features = data.columns[rfe5.support_]\n",
        "print(f'Top-{len(top_5_features)} features selected by RFE: {list(top_5_features)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAvVKNr0c7b9",
        "outputId": "220439e7-b1d0-49a2-d274-a7d9ffc57a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-5 features selected by RFE: ['X1', 'X3', 'X4', 'X5', 'X6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the training and test sets with the selected features\n",
        "X_train_rfe5 = rfe5.transform(X_train)\n",
        "X_test_rfe5 = rfe5.transform(X_test)"
      ],
      "metadata": {
        "id": "Gi83nSKDk8NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning using HyperOpt for XGBClassifier  # Also provide the same seeds  #\n",
        "def hyperopt_train_test(params):\n",
        "    clf = XGBClassifier(**params)\n",
        "    return cross_val_score(clf, X_train_rfe5, y_train.values.ravel(), scoring='recall').mean()\n",
        "\n",
        "space4xgb = {\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 1, 20, 1)),\n",
        "    'n_estimators': scope.int(hp.quniform('n_estimators', 10, 200, 1)),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n",
        "}"
      ],
      "metadata": {
        "id": "l7CoxNYYqfqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(params):\n",
        "    acc = hyperopt_train_test(params)\n",
        "    return {'loss': -acc, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(f, space4xgb, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "print('Best hyperparameters:', best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlNdxmSCqh3a",
        "outputId": "ac596bd4-d617-444f-8939-c0517ac4d6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [00:09<00:00,  5.30trial/s, best loss: -0.6872727272727273]\n",
            "Best hyperparameters: {'learning_rate': 0.021439976728762893, 'max_depth': 7.0, 'n_estimators': 28.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb5 = XGBClassifier(**best_params)\n",
        "model_xgb5.fit(X_train_rfe5, y_train.values.ravel())\n",
        "predictions_xgb5 = model_xgb5.predict(X_test_rfe5)"
      ],
      "metadata": {
        "id": "q7MzLPvYqj1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate XGBClassifier\n",
        "accuracy_xgb5 = accuracy_score(y_test, predictions_xgb5)\n",
        "conf_matrix_xgb5 = confusion_matrix(y_test, predictions_xgb5)\n",
        "class_report_xgb5 = classification_report(y_test, predictions_xgb5)"
      ],
      "metadata": {
        "id": "vaQR1mE5qlfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'XGBClassifier Accuracy: {accuracy_xgb5}')\n",
        "print('XGBClassifier Confusion Matrix:')\n",
        "print(conf_matrix_xgb5)\n",
        "print('XGBClassifier Classification Report:')\n",
        "print(class_report_xgb5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej2tjlI8qnIi",
        "outputId": "91f5a94f-067e-45bd-eae0-ff73f5f0a65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier Accuracy: 0.5769230769230769\n",
            "XGBClassifier Confusion Matrix:\n",
            "[[ 4  4]\n",
            " [ 7 11]]\n",
            "XGBClassifier Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.50      0.42         8\n",
            "           1       0.73      0.61      0.67        18\n",
            "\n",
            "    accuracy                           0.58        26\n",
            "   macro avg       0.55      0.56      0.54        26\n",
            "weighted avg       0.62      0.58      0.59        26\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4th Conclusion\n",
        "\n",
        "1. Once the model works well, remember the seed, the methods and model(classifier) to run for several times.\n",
        "=> In this case, random_state = 6352, XGBClassifier using Top-3 features for Recursive Feature Elimination worked the best.\n",
        "\n",
        "\n",
        "# !Final conclusion!\n",
        "[Task Interpretation]\n",
        "1. Coming back to interpreting the task, unsatisfactory customers were unsatisfied  due to the feature X1, X5, and X6, each indicating the order wasn't delievered on time, didn't satisfy with courier and using the app (maybe app ui wasn't user-frindely) might inconvenient for customers. I figured it out with using XGBClassifier which shows 75% of accuracy.\n",
        "\n",
        "\n",
        "[What I learned]\n",
        "Logical Thinking <br>\n",
        "1. When model works well, or do not work at all, I should approach with deeper thinking to understand and interpret the result. Try to be more specific and practicing alone by explaining things to others will help me improve logical thinking.\n",
        "\n",
        "Time Management <br>\n",
        "2. In order to do that, I should manage my time more wisely."
      ],
      "metadata": {
        "id": "9OFzpkHhPhr4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtT9z6gYt7AQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}